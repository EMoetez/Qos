# QOS Data Pipeline Project

This project processes Quality of Service (QoS) data using NiFi, Spark, Elasticsearch, and Kibana, all orchestrated with Docker Compose.

## Prerequisites

*   Docker & Docker Compose
*   Git
*   Bash Shell (Linux/macOS, or Git Bash/WSL on Windows)
*   `curl`, `grep`, `awk`, `sed` (standard utilities)
*   Sufficient host resources (8GB+ RAM recommended)

## Project Structure Highlights

*   `docker-compose.yml`: Defines all services.
*   `start-stack.sh`: **Primary script to start the entire stack.**
*   `.env`: Auto-generated by `start-stack.sh` for the Kibana token.
*   `data/`: Host directory for various data stages (staging, processed, etc.).
*   `kibana-init/kibana-objects.ndjson`: **Crucial file - place your exported Kibana dashboards, visualizations, and data views here.**
*   `spark_service/`: Contains the PySpark Flask application for data processing.

## Quick Start

1.  **Clone the Repository:**
    ```bash
    git clone <your_repository_url>
    cd qos-data-pipeline
    ```

2.  **Prepare Kibana Objects:**
    *   Export your dashboard(s), visualizations, and related data view(s) from your development Kibana instance.
    *   Save the exported file as `kibana-init/kibana-objects.ndjson`. **This step is mandatory for dashboards to load.**

3.  **Set Host Directory Permissions (One-time for development):**
    The `start-stack.sh` script will attempt this, but you might need to run it with `sudo` or do it manually once if it fails due to permissions:
    ```bash
    mkdir -p ./data/staging ./data/landing ./data/processed ./data/archive ./data/error
    sudo chmod -R 777 ./data 
    ```
    *Note: `777` is for development ease; use more secure permissions for production. The `docker-compose.yml` uses the `:z` flag on volumes for SELinux compatibility on Linux hosts.*

4.  **Run the Startup Script:**
    ```bash
    chmod +x start-stack.sh
    ./start-stack.sh
    ```
    This script will:
    *   Attempt to set permissions on `./data/`.
    *   Start Elasticsearch and wait for it to be healthy.
    *   Generate a Kibana service account token.
    *   Write the token to an `.env` file (which `docker-compose` will use).
    *   Start all other services (`docker-compose up -d --build`).

## Accessing Services

Once the stack is fully up (monitor `docker-compose logs -f` or wait a few minutes):

*   **NiFi UI:** `https://localhost:8443/nifi` (Default: `admin`/`adminadmin123`)
*   **Kibana UI:** `http://localhost:5601`
*   **Elasticsearch API:** `http://localhost:9200` (Auth: `elastic`/`elasticelastic123`)
*   **Spark Master UI:** `http://localhost:8081`
*   **Spark Application UI (for active jobs):** `http://localhost:4041`

## Data Flow Overview

1.  Data is ingested by **NiFi**.
2.  NiFi sends batches to the **`spark-service`** API (`/process`).
3.  `spark-service` uses the **Spark cluster** for transformations (e.g., creating `location` data).
4.  Processed data (NDJSON) is returned to **NiFi**.
5.  NiFi indexes the data into **Elasticsearch** and can also write it to `./data/processed/`.
6.  **Kibana** visualizes data from Elasticsearch using pre-loaded dashboards.

## Stopping the Stack

To stop all services:
```bash
docker-compose down


> **Note**  
> The `./data` directory on your host remains unless manually deleted.

## ðŸ›  Troubleshooting

- **Logs**  
  View logs using:  
  ```bash
  docker-compose logs -f <service_name>
