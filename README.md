<h1>Intelligent QoS Management & Predictive Analytics for Telecom Networks</h1>

<p>An end-to-end data pipeline using Apache NiFi, Apache Spark, Elasticsearch, and Kibana to provide AI-driven Quality of Service (QoS) monitoring, forecasting, and anomaly detection for telecommunication networks. This project was developed as a Final Year Project in collaboration with Orange Tunisie.</p>

<p align="center">
  <img src="path/to/your/architecture_diagram.png" alt="High-Level Architecture" width="800">
  <br>
  <em><b>Note:</b> You must create and link your own architecture diagram here.</em>
</p>

<hr>

<h2> Features</h2>

<ul>
  <li><b>Automated Data Pipeline:</b> Ingests raw QoS data files using <b>NiFi</b>, processes them in a distributed manner with <b>Spark</b>, and indexes them into <b>Elasticsearch</b>.</li>
  <li><b>AI-Powered Insights:</b>
    <ul>
      <li><b>Time Series Forecasting:</b> Utilizes models like ARIMA, SARIMA, Prophet, and LSTM to predict key KPIs such as user load.</li>
      <li><b>Unsupervised Anomaly Detection:</b> Employs Isolation Forest and Autoencoders to identify unusual patterns and potential network faults.</li>
    </ul>
  </li>
  <li><b>Experiment Tracking:</b> Leverages <b>MLflow</b> for rigorous tracking of all AI model experiments, including parameters, metrics, and artifacts.</li>
  <li><b>Interactive Visualization:</b>
    <ul>
      <li><b>Kibana:</b> Operational dashboards for real-time QoS monitoring, KPI trends, and geospatial analysis.</li>
      <li><b>Streamlit:</b> Dedicated dashboards for in-depth analysis and comparison of AI model performance, directly loading artifacts from MLflow.</li>
    </ul>
  </li>
  <li><b>Containerized & Automated Deployment:</b> The entire stack is containerized with <b>Docker</b> and orchestrated with <b>Docker Compose</b>. A <code>start-stack.sh</code> script automates the entire startup process, including dynamic Kibana token generation.</li>
</ul>

<hr>

<h2>️ Technology Stack</h2>

<table width="100%">
  <thead>
    <tr>
      <th>Category</th>
      <th>Technologies & Tools</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>Data Ingestion & Flow</b></td>
      <td>Apache NiFi</td>
    </tr>
    <tr>
      <td><b>Distributed Processing</b></td>
      <td>Apache Spark (PySpark)</td>
    </tr>
    <tr>
      <td><b>Data Storage & Indexing</b></td>
      <td>Elasticsearch</td>
    </tr>
    <tr>
      <td><b>Visualization & Reporting</b></td>
      <td>Kibana, Streamlit, Matplotlib, Seaborn</td>
    </tr>
    <tr>
      <td><b>AI / Machine Learning</b></td>
      <td>Scikit-learn, TensorFlow/Keras, Statsmodels, Prophet, Pandas, NumPy</td>
    </tr>
    <tr>
      <td><b>MLOps & Deployment</b></td>
      <td>MLflow, Docker, Docker Compose, Bash Scripting</td>
    </tr>
    <tr>
      <td><b>API & Backend</b></td>
      <td>Flask</td>
    </tr>
    <tr>
      <td><b>Version Control</b></td>
      <td>Git, GitHub</td>
    </tr>
  </tbody>
</table>

<hr>

<h2> Project Structure</h2>

<pre><code>.
├── docker-compose.yml          # Main Docker Compose configuration
├── start-stack.sh              # Primary script to start the entire stack
├── .env                        # (Auto-generated by start-stack.sh for Kibana token)
├── data/                       # Host directory for persistent data
│   ├── staging/
│   ├── landing/
│   ├── processed/
│   └── ...
├── init/                       # Docker context for Elasticsearch index creation
├── kibana-init/                # Docker context for Kibana dashboard import
│   └── kibana-objects.ndjson   # <-- IMPORTANT: Your exported Kibana objects go here
├── nifi-init/                  # Docker context for custom NiFi setup
├── spark_service/              # Docker context for the PySpark Flask application
└── ...
</code></pre>

<hr>

<h2>⚙️ Setup and Installation</h2>

<h3>Prerequisites</h3>
<ul>
  <li>Docker & Docker Compose</li>
  <li>Git</li>
  <li>A Bash-compatible shell (Linux, macOS, Git Bash on Windows)</li>
  <li><code>curl</code> utility</li>
</ul>

<h3>1. Clone the Repository</h3>
<pre><code>git clone <your_repository_url>
cd <your_repository_directory>
</code></pre>

<h3>2. Prepare Kibana Objects (Mandatory)</h3>
<p>You must export your dashboards, visualizations, and data views from your development Kibana instance.</p>
<ul>
  <li>Save the exported file as <code>kibana-init/kibana-objects.ndjson</code>.</li>
  <li><b>Ensure you include all related objects during the export.</b></li>
</ul>

<h3>3. Set Host Directory Permissions</h3>
<p>For Docker containers to write to the <code>./data</code> directory on your host, you may need to set permissions. The <code>start-stack.sh</code> script will attempt to do this.</p>
<pre><code># This is handled by the start script, but can be run manually if needed
sudo chmod -R 777 ./data
</code></pre>
<p><i>Note: This is for development convenience. The <code>docker-compose.yml</code> also uses the <code>:z</code> flag for SELinux compatibility.</i></p>

<hr>

<h2>▶️ Running the Stack</h2>

<p>The <code>start-stack.sh</code> script automates the entire startup sequence, including generating the dynamic Kibana token.</p>

<p><b>1. Make the script executable:</b></p>
<pre><code>chmod +x start-stack.sh
</code></pre>

<p><b>2. Run the script:</b></p>
<pre><code>./start-stack.sh
</code></pre>

<hr>

<h2> Accessing Services</h2>

<p>Once the stack is running, services can be accessed at the following URLs:</p>
<ul>
  <li><b>NiFi UI:</b> <code>https://localhost:8443/nifi</code></li>
  <li><b>Kibana UI:</b> <code>http://localhost:5601</code></li>
  <li><b>Elasticsearch API:</b> <code>http://localhost:9200</code></li>
  <li><b>Spark Master UI:</b> <code>http://localhost:8081</code></li>
  <li><b>Spark Application UI (for active jobs):</b> <code>http://localhost:4041</code></li>
  <li><b>MLflow UI (if running as a service):</b> <code>http://localhost:5000</code> (or configured port)</li>
  <li><b>Streamlit Dashboards:</b> <code>http://localhost:8501</code> / <code>http://localhost:8502</code> (depending on how you run them)</li>
</ul>

<hr>

<h2>⏹️ Stopping the Stack</h2>

<p>To stop all services and remove the containers:</p>
<pre><code>docker-compose down
</code></pre>

<p>To perform a full cleanup, including removing named volumes (<b>WARNING:</b> deletes Elasticsearch data, NiFi state, etc.):</p>
<pre><code>docker-compose down -v
rm -f .env # Also remove the auto-generated environment file
</code></pre>

<hr>

<h2> Key AI System Components</h2>
<ul>
  <li><b>Comparative Model Evaluation:</b> The project includes scripts to train and evaluate multiple forecasting (ARIMA, Prophet, LSTM) and anomaly detection (Isolation Forest, Autoencoder) models on a per-governorate basis.</li>
  <li><b>MLflow Tracking:</b> All experiments are tracked in MLflow, logging parameters, metrics (RMSE), and artifacts (models, plots, prediction files).</li>
  <li><b>Streamlit Dashboards for Analysis:</b> Interactive dashboards allow for the visual comparison of AI model performance by loading artifacts directly from MLflow.</li>
</ul>

<hr>

<h2> Future Work</h2>
<ul>
  <li>Operationalize the AI Inference Service to write live predictions back to Elasticsearch.</li>
  <li>Implement a real-time alerting module based on AI insights.</li>
  <li>Transition the pipeline to a streaming architecture for lower latency.</li>
  <li>Mature MLOps practices with automated model retraining and drift detection.</li>
</ul>
